{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmit-ir/Tutotrial-Practical-LLMs/blob/main/LLM_OpenRouter_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeajlZWW4aQ5"
      },
      "source": [
        "# Install and import the necessary packages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   ir_datasets (https://ir-datasets.com/) provides a collection of standard datasets for IR tasks\n",
        "*   Helps calculate Krippendorff's alpha to evaluate the consistency of ratings or annotations\n",
        "\n"
      ],
      "metadata": {
        "id": "GzK9TLtvGgTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ux3GZINosj8-",
        "outputId": "f5f1b829-b915-4a8b-ea38-40e13f520594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ir_datasets in /usr/local/lib/python3.11/dist-packages (0.5.10)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.13.3)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.5.3)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.67.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.4.3)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (18.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (2025.1.31)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
            "Requirement already satisfied: krippendorff in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy<3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from krippendorff) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install ir_datasets\n",
        "%pip install krippendorff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Allows you to send HTTP requests, useful for interacting with APIs or web scraping\n",
        "import json  # Provides methods for working with JSON data\n",
        "import pandas as pd  # Provides data structures and functions needed to manipulate structured data\n",
        "import ir_datasets  # A package for handling information retrieval datasets\n",
        "import krippendorff  # A library for calculating Krippendorff's alpha, a measure of agreement or reliability between raters\n",
        "import re  # Provides regular expression operations for text pattern\n",
        "import textwrap  # Used for formatting and wrapping text, useful for displaying text in a readable way\n",
        "import numpy as np  # A package for scientific computing in Python\n",
        "\n",
        "import seaborn as sns  # A data visualization library\n",
        "import matplotlib.pyplot as plt  # A widely used library for creating static, animated, and interactive plots\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Provides tools for evaluating machine learning models, especially confusion matrices\n",
        "from google.colab import userdata  # Provides access to user-specific information and utilities in Google Colab environments"
      ],
      "metadata": {
        "id": "KtyFRuZGH0Dc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "mXpm7q1Asjjt"
      },
      "outputs": [],
      "source": [
        "VERBOSE = 0  # # 'VERBOSE' controls the level of logging or output that is displayed (0: no output, 1: some output, 2: all output)\n",
        "\n",
        "# set line wrap for print, lower for smaller screens\n",
        "WRAP = 100 # Defines the maximum line width for wrapping text\n",
        "printw = lambda x: print(textwrap.fill(x, WRAP)) # Create a lambda function that wraps text to fit within the specified width (WRAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzyC-yWf4kFe"
      },
      "source": [
        "# Load the first dataset and print some basic stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the collection ID of the dataset you want to load\n",
        "collection_id = \"antique/test/non-offensive\"\n",
        "\n",
        "# Load the dataset using the 'ir_datasets' package\n",
        "# This loads the dataset identified by 'collection_id', which contains documents, queries, and relevance judgments\n",
        "dataset = ir_datasets.load(collection_id)\n",
        "\n",
        "\n",
        "# Access the document store (a collection of documents in the dataset)\n",
        "docstore = dataset.docs_store()\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\") # Print a separator line to make the output more readable\n",
        "\n",
        "# Print the number of documents in the dataset\n",
        "# 'docs_count()' retrieves the number of documents in the dataset and formats it with commas for readability\n",
        "print(\n",
        "    f\"Number of documents in {collection_id.capitalize()} dataset: {dataset.docs_count():,d}\"\n",
        ")\n",
        "\n",
        "# Print the number of queries in the dataset\n",
        "print(\n",
        "    f\"Number of queries in {collection_id.capitalize()} dataset: {dataset.queries_count():,d}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Print the number of qrels (query-document relevance judgments) in the dataset\n",
        "print(\n",
        "    f\"Number of qrels in full {collection_id.capitalize()} dataset: {dataset.qrels_count():,d}\"\n",
        ")\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\") # Print a separator line to make the output more readable\n",
        "\n",
        "# Handling the loading and display of qrels (query-document relevance judgments)\n",
        "try:\n",
        "    # Attempt to load qrels and show the first few rows if available\n",
        "    qrels_df = pd.DataFrame(list(dataset.qrels_iter()))\n",
        "    print(\"\\nFirst few rows of qrels:\")\n",
        "    print(qrels_df.head())  # Display the first few rows of qrels for verification\n",
        "except AttributeError: # If the dataset does not have qrels, handle the exception\n",
        "    print(\"The dataset does not seem to have qrels.\")\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\") # Print a separator line to make the output more readable"
      ],
      "metadata": {
        "id": "4GMtnufOJU9u",
        "outputId": "9c1c95ed-3488-44c0-8ea0-0746d0ed17cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "Number of documents in Antique/test/non-offensive dataset: 403,666\n",
            "Number of queries in Antique/test/non-offensive dataset: 176\n",
            "Number of qrels in full Antique/test/non-offensive dataset: 5,752\n",
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "\n",
            "First few rows of qrels:\n",
            "  query_id      doc_id  relevance iteration\n",
            "0  1964316   1964316_5          4        U0\n",
            "1  1964316  1674088_11          1        Q0\n",
            "2  1964316  1218838_13          2        Q0\n",
            "3  1964316  1519022_15          2        Q0\n",
            "4  1964316   3059341_5          2        Q0\n",
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create new dataframes for further analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "rKApY_1GQeV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for queries by iterating through all queries in the dataset\n",
        "queries_df = pd.DataFrame(dataset.queries_iter()).set_index(\"query_id\")\n",
        "\n",
        "# Convert the query_id index to string type\n",
        "queries_df.index = queries_df.index.astype(str)\n",
        "\n",
        "# Create a DataFrame for relevance judgments (qrels) by iterating through all qrels in the dataset\n",
        "qrels_df = pd.DataFrame(dataset.qrels_iter())"
      ],
      "metadata": {
        "id": "ZbeAYGKUKUfF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a small portion of queries (20) and documents for inspection of the dataset"
      ],
      "metadata": {
        "id": "L8UfLkM-Qp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\", \"--*--\" * 15, \"\\n\") # Print a separator line to make the output more readable\n",
        "\n",
        "# Set the number of queries to select\n",
        "qn = 20\n",
        "print(f\"For this example we select only the first {qn} queries\")\n",
        "\n",
        "# Select the first 20 rows from the 'queries_df' DataFrame to work with only a subset of queries\n",
        "queries_df = queries_df[:qn]\n",
        "\n",
        "# Filter the relevance judgments (qrels) to only include those with query_ids in the selected queries\n",
        "qrels_df = qrels_df.loc[qrels_df[\"query_id\"].isin(queries_df.index)]\n",
        "\n",
        "# Set the number of documents to sample per query\n",
        "dn = 5\n",
        "print(f\"For this example we sample only {dn} documents per query\")\n",
        "\n",
        "# Sample a fixed number of documents per query (without replacement) from the filtered qrels\n",
        "qrels_df = qrels_df.groupby(\"query_id\").sample(n=dn, replace=False, random_state=2703)\n",
        "\n",
        "# Print the number of queries and qrels after filtering and sampling\n",
        "print(f\"Number of queries after filtering: {len(queries_df)}\")\n",
        "print(f\"Number of qrels after filtering: {len(qrels_df)}\")\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")"
      ],
      "metadata": {
        "id": "QVwuQlJtLKAD",
        "outputId": "5a428826-a8f6-4646-8e7d-bfbda5200c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "For this example we select only the first 20 queries\n",
            "For this example we sample only 5 documents per query\n",
            "Number of queries after filtering: 20\n",
            "Number of qrels after filtering: 100\n",
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-4r7HsCO-3z",
        "outputId": "5a7afdb7-4e8e-4ff5-ea67-e19efbc3ce3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "Queries dataframe:\n",
            "                                                       text\n",
            "query_id                                                   \n",
            "1287437                  how can i lose 30 pounds by june3?\n",
            "1351675   Why must I have an uncracked winshield in orde...\n",
            "1702151                       How patient a driver are you?\n",
            "1783010                                  What is Blaphsemy?\n",
            "1844896                          How to cook Angus burgers?\n",
            "1880028   What does \"see Leaflet\" mean on Ept Pregnancy ...\n",
            "2142044   What do you speculate happened to Natalie Holl...\n",
            "2484180    why do people judge a dog by what he looks like?\n",
            "2528767   How do I determine the charge of the iron ion ...\n",
            "2799913   What's the proper way to express sorrow at a f...\n",
            "2814599   what does the word remission mean when referri...\n",
            "2956570   What are the words to write the sound of raind...\n",
            "3698636                               why do cats headbutt?\n",
            "3990512           how can we get concentration onsomething?\n",
            "4448097                            what is innate immunity?\n",
            "676028    Why do our elected federal senators and congre...\n",
            "707303    How do people get hiccups, what are they, and ...\n",
            "714612    Why doesn't the water fall off  earth if it's ...\n",
            "821387       I have mice.How do I get rid of them humanely?\n",
            "851124         How is ego a part of our survival instincts?\n",
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "Qrels dataframe:\n",
            "     query_id      doc_id  relevance iteration\n",
            "4335  1287437   2489233_8          4        Q0\n",
            "4326  1287437   956682_15          4        Q0\n",
            "4314  1287437    671806_0          2        Q0\n",
            "4324  1287437   1287437_4          3        Q0\n",
            "4342  1287437   1768249_5          1        Q0\n",
            "...       ...         ...        ...       ...\n",
            "2193   851124   4320631_5          1        Q0\n",
            "2167   851124  2762948_14          2        E0\n",
            "2176   851124   3964333_5          2        Q0\n",
            "2191   851124    279279_3          1        Q0\n",
            "2170   851124  4271823_11          1        Q0\n",
            "\n",
            "[100 rows x 4 columns]\n",
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")\n",
        "print(\"Queries dataframe:\")\n",
        "print(queries_df.sort_index()) # Sort the queries dataframe by its index (query_id) in ascending order\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")\n",
        "print(\"Qrels dataframe:\")\n",
        "print(qrels_df)\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print the definitions of the relevance labels between 1 and 4"
      ],
      "metadata": {
        "id": "7LUu5W0gSt45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrels_def = \"\".join(\n",
        "    [textwrap.fill(f\"{k}: {v}\", WRAP) + \"\\n\" for k, v in dataset.qrels_defs().items()]\n",
        ")\n",
        "\n",
        "print(\"Qrels definitions:\\n\")\n",
        "print(qrels_def) # Print the formatted qrels definitions"
      ],
      "metadata": {
        "id": "V9J3ppreSrl6",
        "outputId": "b41b2d66-8d9e-49d9-b859-06a39fff1aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qrels definitions:\n",
            "\n",
            "4: It looks reasonable and convincing. Its quality is on parwith or better than the \"Possibly\n",
            "Correct Answer\". Note that it does not have to provide the same answer as the \"PossiblyCorrect\n",
            "Answer\".\n",
            "3: It can be an answer to the question, however, it is notsufficiently convincing. There should be\n",
            "an answer with much better quality for the question.\n",
            "2: It does not answer the question or if it does, it provides anunreasonable answer, however, it is\n",
            "not out of context. Therefore, you cannot accept it as an answer to the question.\n",
            "1: It is completely out of context or does not make any sense.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check relevance labels in the dataset"
      ],
      "metadata": {
        "id": "HxRLM7kmTVA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the range of relevance values and find the minimum and maximum relevance scores in the dataset\n",
        "min_rel = qrels_df[\"relevance\"].min()  # Get the lowest relevance value\n",
        "max_rel = qrels_df[\"relevance\"].max()  # Get the highest relevance value\n",
        "\n",
        "# Print the min and max relevance values for reference\n",
        "print(f\"The minimum relevance in the dataset is: {min_rel} and maximum relevance: {max_rel}\\n\")\n",
        "\n",
        "\n",
        "# Plot the distribution of relevance values using a histogram\n",
        "sns.histplot(\n",
        "    qrels_df.sort_values(\"relevance\").astype(str),  # Sort and convert to string for visualization\n",
        "    x=\"relevance\",  # Specify 'relevance' as the x-axis variable\n",
        "    discrete=True    # Treat relevance values as discrete categories\n",
        ")\n",
        "\n",
        "# Display the histogram plot\n",
        "plt.show()\n",
        "\n",
        "# Count occurrences of each relevance value and sort them in ascending order\n",
        "qrels_df[\"relevance\"].value_counts().sort_index()"
      ],
      "metadata": {
        "id": "Lx9B_T8KTeeK",
        "outputId": "62521186-b288-45a0-ab64-8eb65619af77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum relevance in the dataset is: 1 and maximum relevance: 4\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "relevance\n",
              "1    27\n",
              "2    37\n",
              "3    20\n",
              "4    16\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relevance</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPRKVRAs2Crd",
        "outputId": "e28e19de-94aa-490b-e9f6-3ee538b90dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "Random row from qrels:\n",
            "    query_id    doc_id  relevance iteration\n",
            "863  4448097  827475_3          1        Q0\n",
            "The relevance of document 827475_3 to query 4448097 is: 1\n",
            "\n",
            " --*----*----*----*----*----*----*----*----*----*----*----*----*----*----*-- \n",
            "\n",
            "Query 4448097:\n",
            "what is innate immunity? \n",
            "\n",
            "Document 827475_3:\n",
            "Document's text: I believe that being cold lowers your immune system and since your immune system is\n",
            "what you need to fight colds, you are at higher risk for catching something when it is cold outside\n",
            "and you have a lot of exposure to those elements.\n"
          ]
        }
      ],
      "source": [
        "# Print one random relevance judgment from Qrels\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")\n",
        "random_qrel_row = qrels_df.sample(n=1, random_state=2703)\n",
        "print(f\"Random row from qrels:\\n{random_qrel_row}\")\n",
        "qid, doc_id, relevance, _ = random_qrel_row.to_numpy()[0]\n",
        "\n",
        "print(f\"The relevance of document {doc_id} to query {qid} is: {relevance}\")\n",
        "\n",
        "# Print the query and document from the relevance judgment\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")\n",
        "print(f\"Query {qid}:\")\n",
        "print(queries_df.loc[qid, \"text\"], \"\\n\")\n",
        "\n",
        "doc = docstore.get(doc_id)\n",
        "print(f\"Document {doc.doc_id}:\")\n",
        "printw(f\"Document's text:\\n{doc.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwUUeALR4zH6"
      },
      "source": [
        "# Create instructions prompts\n",
        "Create a relevance judgment prompt template. The prompt is a template that will\n",
        "be filled with the query and document text. Note using an f-string with `{variable}` that will be filled\n",
        "immediately, and `{{variable}}` that will be filled in later. The prompt template should look like this:\n",
        "```python\n",
        "prompt_template = \"\"\"\n",
        "You are [...]\n",
        "Query: {query}\n",
        "Document: {document}\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zs6Tauj5caB"
      },
      "outputs": [],
      "source": [
        "_prompt = f\"\"\"Judge the relevance of the following document to the given query.\n",
        "Output a relevance score based on the following scale:\n",
        "\n",
        "{qrels_def}\n",
        "If the document is completely irrelevant to the query, output {min_rel}.\n",
        "Otherwise, output a relevance score between {int(min_rel) + 1} and {max_rel}.\n",
        "\n",
        "The response should be a single number between {min_rel} and {max_rel}.\n",
        "Provide only a single score without any additional text.\n",
        "\n",
        "Query:\n",
        "{{query}}\n",
        "\n",
        "Document:\n",
        "{{document}}\n",
        "\"\"\"\n",
        "print(\"Prompt template:\")\n",
        "print(_prompt)\n",
        "\n",
        "print(\"\\n\", \"--*--\" * 15, \"\\n\")\n",
        "print(\"An example of the prompt with a query and document:\")\n",
        "print(\n",
        "    _prompt.format(\n",
        "        query=queries_df.iloc[0].text,\n",
        "        document=textwrap.fill(docstore.get(qrels_df.iloc[0].doc_id).text),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5MgUlH64_u0"
      },
      "source": [
        "# Define functions for model api calls and response parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LReDtNrqFxN"
      },
      "outputs": [],
      "source": [
        "def get_response(prompt: str, model: str, verbose: int = VERBOSE, **model_kwargs) -> dict:\n",
        "    \"\"\"\n",
        "    Get a response from the OpenRouter API using the given prompt and model.\n",
        "    Make sure to set your OpenRouter API key in the environment variable\n",
        "    OPENROUTER_API_KEY. OpenRouter normalizes requests and responses across\n",
        "    providers. That is, you can use the same code to call different models from\n",
        "    different providers.\n",
        "    Args:\n",
        "        prompt (str): The prompt to send to the model.\n",
        "        model (str): The model to use.\n",
        "        verbose (int): Verbosity level for debugging.\n",
        "        **model_kwargs: Additional keyword arguments for the model.\n",
        "            - top_p: Top-p sampling parameter.\n",
        "            - temperature: Temperature parameter for sampling.\n",
        "            - frequency_penalty: Frequency penalty parameter.\n",
        "            - presence_penalty: Presence penalty parameter.\n",
        "            - repetition_penalty: Repetition penalty parameter.\n",
        "            - top_k: Top-k sampling parameter.\n",
        "    Note: The model_kwargs parameters are optional and will be set to default values if not provided.\n",
        "    Returns:\n",
        "        dict: The response from the model.\n",
        "    \"\"\"\n",
        "    # Check if model parameter is provided, if not, set a default value\n",
        "    top_p = model_kwargs.get(\"top_p\", 1)\n",
        "    temperature = model_kwargs.get(\"temperature\", 0.9)\n",
        "    frequency_penalty = model_kwargs.get(\"frequency_penalty\", 0)\n",
        "    presence_penalty = model_kwargs.get(\"presence_penalty\", 0)\n",
        "    repetition_penalty = model_kwargs.get(\"repetition_penalty\", 1)\n",
        "    top_k = model_kwargs.get(\"top_k\", 0)\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\"Authorization\": f\"Bearer {userdata.get('OPENROUTER_API_KEY')}\"},\n",
        "        data=json.dumps(\n",
        "            {\n",
        "                \"model\": model,\n",
        "                \"messages\": messages,\n",
        "                \"top_p\": top_p,\n",
        "                \"temperature\": temperature,\n",
        "                \"frequency_penalty\": frequency_penalty,\n",
        "                \"presence_penalty\": presence_penalty,\n",
        "                \"repetition_penalty\": repetition_penalty,\n",
        "                \"top_k\": top_k,\n",
        "            }\n",
        "        ),\n",
        "    )\n",
        "    if verbose > 0:\n",
        "        print(f\"Response status code: {response.status_code}\")\n",
        "    response_json = response.json()\n",
        "    # let's print how many tokens we used, it can be useful for cost estimation\n",
        "    if verbose > 0:\n",
        "        print(f\"Response usage: {response_json.get('usage')}\")\n",
        "    return response_json\n",
        "\n",
        "\n",
        "def parse_answer_from_text(text: str, verbose: int = VERBOSE) -> str:\n",
        "    \"\"\"\n",
        "    Parse the answer from the text returned by the model.\n",
        "    The function looks for digits in the text and returns the last one found.\n",
        "    If there are multiple digits, it tries to extract the first score\n",
        "    window of up to 5 terms after the word \"score\".\n",
        "    If no digits are found, it returns an empty string.\n",
        "    Args:\n",
        "        text (str): The text to parse.\n",
        "        verbose (int): Verbosity level for debugging.\n",
        "    Returns:\n",
        "        str: The parsed answer.\n",
        "    \"\"\"\n",
        "    score = \"\"\n",
        "    all_digits = re.findall(r\"\\d+\", text)\n",
        "    if all_digits:\n",
        "        score = all_digits[-1]\n",
        "    else:\n",
        "        if verbose > 0:\n",
        "            print(\"Couldn't parse any digits from the response\")\n",
        "            print(f\"all_digits: {all_digits}\")\n",
        "            print(f\"Text: {text} \\nEnd of Text\\n\")\n",
        "            print(\"\\n\", \"-=return=-\" * 10, \"\\n\")\n",
        "        return \"\"\n",
        "    if len(all_digits) > 1:\n",
        "        # if multiple digits try to extract the first score\n",
        "        # window of up to 5 terms after the \"score\"\n",
        "        try:\n",
        "            score_window = \" \".join(text.lower().split(\"score\")[-1].split()[:5])\n",
        "            score = re.findall(r\"\\d+\", score_window)[0]\n",
        "            if verbose > 1:\n",
        "                print(f\"Text: {text} \\nEnd of Text\\n\")\n",
        "                print(f\"score_window: {score_window}\")\n",
        "        except IndexError:\n",
        "            if verbose > 0:\n",
        "                print(\"Couldn't find score window\")\n",
        "                print(f\"Text: {text} \\nEnd of Text\\n\")\n",
        "    # print(f'All digits: {all_digits}')\n",
        "    # print(f'Extracted Score: {score}')\n",
        "    # print('\\n', '-=return=-' * 10, '\\n')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0NsPuPh5O4d"
      },
      "source": [
        "# Generate the relevance predictions for the first dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HQbxDjtnOTuz"
      },
      "outputs": [],
      "source": [
        "# Available models in OpenRouter\n",
        "# Models are categorized as free (marked with \":free\") or paid with varying pricing\n",
        "# Models may change over time - check current availability and pricing at:\n",
        "# https://openrouter.ai/models\n",
        "# Note: Different models have varying token limits and optimal hyperparameter settings\n",
        "\n",
        "MODEL = {\n",
        "    \"llama-free\": \"meta-llama/llama-3.3-70b-instruct:free\",\n",
        "    \"deepseek-r1-free\": \"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
        "    \"deepseek-r1-qwen\": \"deepseek/deepseek-r1-distill-qwen-1.5b\",\n",
        "    \"gemini-flash-2\": \"google/gemini-2.0-flash-001\",\n",
        "    \"gemini-pro-2\": \"google/gemini-2.0-pro-exp-02-05:free\",\n",
        "    \"gemini-flash-2free\": \"google/gemini-2.0-flash-exp:free\",\n",
        "    \"gemma-3-4b\": \"google/gemma-3-4b-it:free\",\n",
        "    \"llama-3.2-1b\": \"meta-llama/llama-3.2-1b-instruct\",\n",
        "    \"gpt-4o-mini\": \"openai/gpt-4o-mini\",\n",
        "}\n",
        "\n",
        "\n",
        "def generate_relevance_predictions(\n",
        "    qrel_df: pd.DataFrame,\n",
        "    docs,\n",
        "    query_df: pd.DataFrame,\n",
        "    prompt: str,\n",
        "    model: str,\n",
        "    verbose: int = VERBOSE,\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Generate relevance predictions using the OpenRouter API.\n",
        "    Args:\n",
        "        qrel_df (DataFrame): The qrel DataFrame.\n",
        "        docs (DataFrame or Lz4FullStore): The document store.\n",
        "        query_df (DataFrame): The query DataFrame.\n",
        "        prompt (str): The prompt template.\n",
        "        model (str): The model to use.\n",
        "        verbose (int): Verbosity level for debugging.\n",
        "    Returns:\n",
        "        dict: The relevance predictions.\n",
        "        dict: The reasoning results.\n",
        "        list: The raw responses from the model.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    reasoning_results = {}\n",
        "    raw_responses = []  # for debugging, logging and backup\n",
        "\n",
        "    for index, row in qrel_df.iterrows():\n",
        "        qid, doc_id = row[\"query_id\"], row[\"doc_id\"]\n",
        "        if isinstance(docs, ir_datasets.indices.lz4_pickle.PickleLz4FullStore):\n",
        "            doc = docs.get(doc_id)\n",
        "        else:\n",
        "            doc = docs.loc[doc_id]\n",
        "        prompt_text = prompt.format(\n",
        "            query=query_df.loc[qid, \"text\"], document=doc.text\n",
        "        )  # adding a query and document to the prompt\n",
        "        if verbose > 0:\n",
        "            print(f\"Running for {qid, doc_id}\")\n",
        "            if verbose > 1:\n",
        "                print(f\"Prompt for {qid, doc_id}:\")\n",
        "                printw(prompt_text)\n",
        "        response_json = get_response(prompt=prompt_text, model=model, verbose=verbose)\n",
        "        raw_responses.append(response_json)\n",
        "        response_message = response_json.get(\"choices\", {0: {\"message\": None}})[0][\n",
        "            \"message\"\n",
        "        ]\n",
        "        if response_message is None:\n",
        "            print(f\"No response message for {qid, doc_id}\")\n",
        "            print(response_json.get(\"error\"))\n",
        "            continue\n",
        "        if verbose > 1:\n",
        "            print(f\"Response for {qid, doc_id}:\")\n",
        "            printw(f\"Response message: {response_message}\")\n",
        "            print(\"\\n\", \"-=\" * 5, \" End of response \", \"=-\" * 5, \"\\n\")\n",
        "        response_content = response_message.get(\"content\", None)\n",
        "        reasoning_results[qid, doc_id] = response_message.get(\"reasoning\", None)\n",
        "        results[qid, doc_id] = response_content\n",
        "    return results, reasoning_results, raw_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsAC_oxI1q2W"
      },
      "outputs": [],
      "source": [
        "# Run the model for the first time\n",
        "# Note: This takes ~2 minutes\n",
        "first_results, first_reasoning_results, first_raw_responses = (\n",
        "    generate_relevance_predictions(\n",
        "        qrels_df,\n",
        "        docstore,\n",
        "        queries_df,\n",
        "        _prompt,\n",
        "        model=MODEL[\"llama-3.2-1b\"],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFOQQebOzwzB"
      },
      "outputs": [],
      "source": [
        "def compute_total_tokens_used(raw_responses: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Compute the total number of tokens used in the responses.\n",
        "    Args:\n",
        "        raw_responses (list): The raw responses from the model.\n",
        "    Returns:\n",
        "        tuple: The total number of input, output, and total tokens used.\n",
        "    \"\"\"\n",
        "    total_input_tokens = sum(\n",
        "        [response_json[\"usage\"][\"prompt_tokens\"] for response_json in raw_responses]\n",
        "    )\n",
        "    print(f\"Total input tokens used: {total_input_tokens:,d}\")\n",
        "    total_output_tokens = sum(\n",
        "        [response_json[\"usage\"][\"completion_tokens\"] for response_json in raw_responses]\n",
        "    )\n",
        "    print(f\"Total output tokens used: {total_output_tokens:,d}\")\n",
        "    total_tokens = sum(\n",
        "        [response_json[\"usage\"][\"total_tokens\"] for response_json in raw_responses]\n",
        "    )\n",
        "    print(f\"Total tokens used: {total_tokens:,d}\")\n",
        "    return total_input_tokens, total_output_tokens, total_tokens\n",
        "\n",
        "\n",
        "compute_total_tokens_used(first_raw_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb87tOZD5XXj"
      },
      "source": [
        "# Evaluate the relevance judgments of the first dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZCnDdNPBJ4Qp"
      },
      "outputs": [],
      "source": [
        "def parse_results_to_df(\n",
        "    results: dict, reasoning_results=None, scores_range=(-1, 4), verbose: int = VERBOSE\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse the results from the model into a DataFrame.\n",
        "    Args:\n",
        "        results (dict): The results from the model.\n",
        "        reasoning_results (dict): The reasoning results from the model.\n",
        "        scores_range (tuple): The range of scores to use.\n",
        "        verbose (int): Verbosity level for debugging.\n",
        "    Returns:\n",
        "        DataFrame: The parsed results as a DataFrame.\n",
        "    \"\"\"\n",
        "    min_score, max_score = scores_range\n",
        "    p_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "    p_df.index = pd.MultiIndex.from_tuples(p_df.index)\n",
        "    p_df.index.names = [\"query_id\", \"doc_id\"]\n",
        "    p_df.columns = [\"predicted\"]\n",
        "    p_df[\"predicted\"] = p_df[\"predicted\"].apply(parse_answer_from_text, args=[verbose])\n",
        "\n",
        "    if reasoning_results is not None:\n",
        "        # fill empty predictions from the reasoning field in raw responses\n",
        "        for index, row in p_df.loc[p_df[\"predicted\"] == \"\"].iterrows():\n",
        "            qid, doc_id = index\n",
        "            if verbose > 1:\n",
        "                print(f\"Empty prediction for {qid, doc_id}\")\n",
        "                print(f\"Raw response text: {results[qid, doc_id]}\\n\")\n",
        "            if reasoning_results[qid, doc_id] is not None:\n",
        "                p_df.loc[index, \"predicted\"] = parse_answer_from_text(\n",
        "                    reasoning_results[qid, doc_id], verbose=verbose\n",
        "                )\n",
        "            else:\n",
        "                if verbose > 0:\n",
        "                    print(f\"No response or reasoning for: {qid, doc_id}\")\n",
        "                    print(f\"Setting min score: {min_score}\")\n",
        "                p_df.loc[index, \"predicted\"] = str(min_score)\n",
        "\n",
        "    p_df.loc[p_df[\"predicted\"].isin({\"\", 0, \"0\", None})] = str(min_score)\n",
        "    return p_df.astype(int).clip(lower=min_score, upper=max_score)\n",
        "\n",
        "\n",
        "first_predictions_df = parse_results_to_df(\n",
        "    first_results, first_reasoning_results, scores_range=(min_rel, max_rel)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr_tg02FLEkE"
      },
      "outputs": [],
      "source": [
        "def evaluate_relevance_predictions(\n",
        "    predictions_df: pd.DataFrame, qrel_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate relevance predictions against ground truth qrels data.\n",
        "\n",
        "    This function compares predicted relevance scores with actual relevance judgments,\n",
        "    providing several evaluation metrics including confusion matrices, Krippendorff's alpha,\n",
        "    and Spearman's correlation. It also evaluates binary relevance classification.\n",
        "\n",
        "    Args:\n",
        "        predictions_df (pd.DataFrame): DataFrame with predicted relevance scores.\n",
        "            Expected to have MultiIndex of (query_id, doc_id) and a 'predicted' column.\n",
        "        qrel_df (pd.DataFrame): Ground truth relevance judgments DataFrame.\n",
        "            Expected to have columns 'query_id', 'doc_id', and 'relevance'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Merged DataFrame containing both predicted and actual relevance scores,\n",
        "                     sorted by predicted scores and actual relevance.\n",
        "\n",
        "    Visualizes:\n",
        "        - Histograms comparing predicted vs. actual relevance distributions\n",
        "        - Confusion matrix of relevance scores\n",
        "        - Binary relevance evaluation (relevant if score > 1)\n",
        "\n",
        "    Metrics:\n",
        "        - Krippendorff's alpha for both ordinal and binary relevance\n",
        "        - Spearman's rank correlation coefficient\n",
        "    \"\"\"\n",
        "\n",
        "    merged_df = pd.merge(\n",
        "        predictions_df,\n",
        "        qrel_df,\n",
        "        left_index=True,\n",
        "        right_on=[\"query_id\", \"doc_id\"],\n",
        "        how=\"inner\",\n",
        "    )\n",
        "    print(f\"Shape of merged df: {merged_df.shape}\")\n",
        "\n",
        "    # Convert the relevance scores to numeric, handling potential errors.\n",
        "    try:\n",
        "        merged_df[\"relevance\"] = pd.to_numeric(merged_df[\"relevance\"])\n",
        "        merged_df[\"predicted\"] = pd.to_numeric(merged_df[\"predicted\"])\n",
        "    except ValueError as e:\n",
        "        print(f\"Error converting relevance scores to numeric: {e}\")\n",
        "        # Handle the error appropriately, e.g., by removing rows with invalid scores or assigning default values.\n",
        "        # For example, you could remove the invalid rows:\n",
        "        # merged_df = merged_df[pd.to_numeric(merged_df['relevance_x'], errors='coerce').notna()]\n",
        "        # merged_df = merged_df[pd.to_numeric(merged_df['relevance_y'], errors='coerce').notna()]\n",
        "\n",
        "    min_score, max_score = merged_df[\"relevance\"].min(), merged_df[\"relevance\"].max()\n",
        "    # plot two histograms side by side of predicted values and actual relevance values\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharex=True, sharey=True)\n",
        "    sns.histplot(\n",
        "        merged_df[\"predicted\"].sort_values().astype(str),\n",
        "        ax=axs[0],\n",
        "        discrete=True,\n",
        "        zorder=10,\n",
        "    )\n",
        "    axs[0].set_title(\"Predicted Relevance\")\n",
        "    sns.histplot(\n",
        "        merged_df[\"relevance\"].sort_values().astype(str),\n",
        "        ax=axs[1],\n",
        "        discrete=True,\n",
        "        zorder=10,\n",
        "    )\n",
        "    axs[1].set_title(\"Actual Relevance\")\n",
        "    # Add grid lines  on y axis\n",
        "    axs[0].grid(\n",
        "        axis=\"y\",\n",
        "        which=\"major\",\n",
        "        color=\"gray\",\n",
        "        alpha=0.2,\n",
        "        linestyle=\"-.\",\n",
        "        linewidth=0.5,\n",
        "        zorder=1,\n",
        "    )\n",
        "    axs[1].grid(\n",
        "        axis=\"y\",\n",
        "        which=\"major\",\n",
        "        color=\"gray\",\n",
        "        alpha=0.2,\n",
        "        linestyle=\"-.\",\n",
        "        linewidth=0.5,\n",
        "        zorder=1,\n",
        "    )\n",
        "\n",
        "    # show tick labels on y axis\n",
        "    axs[1].yaxis.set_tick_params(labelleft=True)\n",
        "\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # plot a confusion matrix\n",
        "    cm = confusion_matrix(merged_df[\"relevance\"], merged_df[\"predicted\"])\n",
        "    disp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix=cm, display_labels=np.sort(merged_df[\"relevance\"].unique())\n",
        "    )\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Compute Krippendorff's alpha. reliability_data expects an array or df where the coders are rows and units are columns\n",
        "    alpha = krippendorff.alpha(\n",
        "        reliability_data=merged_df[[\"relevance\", \"predicted\"]].T,\n",
        "        level_of_measurement=\"ordinal\",\n",
        "    )\n",
        "    print(f\"Krippendorff's alpha: {alpha:.2f}\")\n",
        "\n",
        "    # Compute Spearman's correlation\n",
        "    spearman_corr = merged_df[\"relevance\"].corr(\n",
        "        merged_df[\"predicted\"], method=\"spearman\"\n",
        "    )\n",
        "    print(f\"Spearman's correlation: {spearman_corr:.2f}\")\n",
        "\n",
        "    print(\"---\" * 10, \"Binary Relevance Eval\", \"---\" * 10)\n",
        "    bi_relevance_df = merged_df[[\"predicted\", \"relevance\"]].map(\n",
        "        lambda x: True if x > 1 else False\n",
        "    )\n",
        "    sns.histplot(\n",
        "        data=bi_relevance_df.stack()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"level_1\": \"\", 0: \"label\"})\n",
        "        .astype(str),\n",
        "        x=\"label\",\n",
        "        hue=\"\",\n",
        "        discrete=True,\n",
        "    )\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "    bi_relevance_df.value_counts().to_frame().pivot_table(\n",
        "        index=\"predicted\", columns=\"relevance\"\n",
        "    )\n",
        "    cm = confusion_matrix(bi_relevance_df[\"relevance\"], bi_relevance_df[\"predicted\"])\n",
        "    disp = ConfusionMatrixDisplay(\n",
        "        confusion_matrix=cm,\n",
        "        display_labels=np.sort(bi_relevance_df[\"relevance\"].unique()),\n",
        "    )\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Compute Krippendorff's alpha. reliability_data expects an array or df where the coders are rows and units are columns\n",
        "    alpha = krippendorff.alpha(\n",
        "        reliability_data=bi_relevance_df[[\"relevance\", \"predicted\"]].T.astype(int),\n",
        "        level_of_measurement=\"nominal\",\n",
        "    )\n",
        "\n",
        "    print(f\"Krippendorff's alpha: {alpha:.2f}\")\n",
        "\n",
        "    return merged_df.sort_values(by=[\"predicted\", \"relevance\"])\n",
        "\n",
        "\n",
        "evaluate_relevance_predictions(first_predictions_df, qrels_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNxvKah15jsO"
      },
      "source": [
        "# Quick Summary\n",
        "## We've defined the following utility functions:\n",
        "1. [get_response, parse_answer_from_text](#scrollTo=7LReDtNrqFxN&line=3&uniqifier=1)\n",
        "2. [compute_total_tokens_used](#scrollTo=IFOQQebOzwzB&line=1&uniqifier=1)\n",
        "3. [parse_results_to_df](#scrollTo=ZCnDdNPBJ4Qp&line=1&uniqifier=1)\n",
        "\n",
        "## And the following processing functions:\n",
        "1. [generate_relevance_predictions](#scrollTo=HQbxDjtnOTuz&line=8&uniqifier=1)\n",
        "2. [evaluate_relevance_predictions](#scrollTo=Gr_tg02FLEkE&line=4&uniqifier=1)\n",
        "\n",
        "\n",
        "## Next, we'll load a new dataset and apply these functions on new data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgyVsFxT9N_8"
      },
      "source": [
        "# Loading and predicting the LLM4EVal dataset\n",
        "## https://llm4eval.github.io/LLMJudge-benchmark/#\n",
        "### based on data from TREC-DL 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZGlKYeGOwel"
      },
      "outputs": [],
      "source": [
        "# clone the GitHub repo locally\n",
        "!git clone https://github.com/llm4eval/LLMJudge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgnmFZ-UTadC"
      },
      "outputs": [],
      "source": [
        "trec_docs_df = (\n",
        "    pd.read_json(\"LLMJudge/data/llm4eval_document_2024.jsonl\", lines=True)\n",
        "    .rename(columns={\"docid\": \"doc_id\", \"doc\": \"text\"})\n",
        "    .set_index(\"doc_id\")\n",
        ")\n",
        "trec_queries_df = pd.read_csv(\n",
        "    \"LLMJudge/data/llm4eval_query_2024.txt\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"query_id\", \"text\"],\n",
        ").set_index(\"query_id\")\n",
        "trec_qrels_df = pd.read_csv(\n",
        "    \"LLMJudge/data/llm4eval_dev_qrel_2024.txt\",\n",
        "    sep=\"\\s+\",\n",
        "    header=None,\n",
        "    names=[\"query_id\", \"iteration\", \"doc_id\", \"relevance\"],\n",
        ").drop(columns=\"iteration\")\n",
        "\n",
        "\n",
        "# select first qn queries\n",
        "qn = 15\n",
        "first_n_qids = trec_qrels_df[\"query_id\"].unique()[:qn]\n",
        "trec_queries_df = trec_queries_df.loc[first_n_qids]\n",
        "trec_qrels_df = trec_qrels_df.loc[trec_qrels_df[\"query_id\"].isin(first_n_qids)]\n",
        "\n",
        "# sample dn documents per query\n",
        "dn = 10\n",
        "print(f\"Sampling {dn} documents per query\")\n",
        "trec_qrels_df = trec_qrels_df.groupby(\"query_id\").sample(\n",
        "    n=dn, replace=False, random_state=2703\n",
        ")\n",
        "\n",
        "print(f\"Number of selected queries in TREC dataset: {len(trec_queries_df)}\")\n",
        "print(f\"Number of selected qrels in TREC dataset: {len(trec_qrels_df)}\")\n",
        "\n",
        "trec_min_rel = trec_qrels_df[\"relevance\"].min()\n",
        "trec_max_rel = trec_qrels_df[\"relevance\"].max()\n",
        "print(f\"Min relevance: {min_rel}, Max relevance: {max_rel}\\n\")\n",
        "print(\n",
        "    \"\"\"Relevance definitions:\n",
        "[3] Perfectly relevant: The passage is dedicated to the query and contains the exact answer.\n",
        "[2] Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.\n",
        "[1] Related: The passage seems related to the query but does not answer it.\n",
        "[0] Irrelevant: The passage has nothing to do with the query.\\n\"\"\"\n",
        ")\n",
        "\n",
        "sns.histplot(\n",
        "    trec_qrels_df.sort_values(\"relevance\").astype(str),\n",
        "    x=\"relevance\",\n",
        "    discrete=True,\n",
        "    binrange=(trec_min_rel, trec_max_rel),\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "trec_qrels_df[\"relevance\"].value_counts()\n",
        "\n",
        "trec_qrels_df.loc[trec_qrels_df[\"query_id\"].isin(first_n_qids)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2sP9n6vT8XJ"
      },
      "outputs": [],
      "source": [
        "# define TREC instructions prompt\n",
        "\n",
        "first_prompt = f\"\"\"Judge the relevance of the below document to the given query.\n",
        "Output a relevance score based on the following scale:\n",
        "[3] Perfectly relevant: The passage is dedicated to the query and contains the exact answer.\n",
        "[2] Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.\n",
        "[1] Related: The passage seems related to the query but does not answer it.\n",
        "[0] Irrelevant: The passage has nothing to do with the query.\n",
        "\n",
        "The response should be a single number between {trec_min_rel} and {trec_max_rel}.\n",
        "\n",
        "The thought process should be first to decide whether the document is relevant or not,\n",
        "if not, assign it a {trec_min_rel}.\n",
        "Otherwise decide how relevant the document is to the query, based on that assign a score between {int(trec_min_rel) + 1} and {trec_max_rel}.\n",
        "{trec_max_rel} should be assigned only to perfectly relevant documents.\n",
        "\n",
        "Provide only a single score without any additional text.\n",
        "\n",
        "Query:\n",
        "{{query}}\n",
        "\n",
        "Document:\n",
        "{{document}}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Upadhyay, S., Pradeep, R., Thakur, N., Craswell, N., & Lin, J. (2024).\n",
        "# Umbrela: Umbrela is the (open-source reproduction of the) bing relevance assessor. arXiv preprint arXiv:2406.06519.\n",
        "umbrella_prompt = \"\"\"Given a query and a passage, you must provide a score on an integer scale of 0 to 3 with the\n",
        "following meanings:\n",
        "\n",
        "0 = represent that the passage has nothing to do with the query,\n",
        "1 = represents that the passage seems related to the query but does not answer it,\n",
        "2 = represents that the passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous\n",
        "information and\n",
        "3 = represents that the passage is dedicated to the query and contains the exact answer.\n",
        "\n",
        "Important Instruction: Assign category 1 if the passage is somewhat related to the topic but not completely,\n",
        "category 2 if the passage presents something very important related to the entire\n",
        "topic but also has some extra information and category 3 if the passage only and entirely refers to\n",
        "the topic. If none of the above satisfies give it category 0.\n",
        "\n",
        "Query: {query}\n",
        "Passage: {document}\n",
        "\n",
        "Split this problem into steps: Consider the underlying intent of the search. Measure how well the content\n",
        "matches a likely intent of the query (M). Measure how trustworthy the passage is (T). Consider the\n",
        "aspects above and the relative importance of each, and decide on a final score (O).\n",
        "\n",
        "Final score must be an integer value only. Do not provide any code in the result.\n",
        "Provide each score in the format of:\n",
        "##final score: score without providing any reasoning.\n",
        "\"\"\"\n",
        "print(umbrella_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o8OrEWeN0Y-k"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the TREC data - Takes 2-4 minutes\n",
        "trec_results, trec_reasoning_results, trec_raw_responses = (\n",
        "    generate_relevance_predictions(\n",
        "        trec_qrels_df,\n",
        "        trec_docs_df,\n",
        "        trec_queries_df,\n",
        "        prompt=first_prompt,\n",
        "        model=MODEL[\"llama-3.2-1b\"],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QzHXN-eQ-uEB"
      },
      "outputs": [],
      "source": [
        "compute_total_tokens_used(trec_raw_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISD-cqWrT6Dr"
      },
      "outputs": [],
      "source": [
        "trec_predictions_df = parse_results_to_df(\n",
        "    trec_results,\n",
        "    trec_reasoning_results,\n",
        "    scores_range=(trec_min_rel, trec_max_rel),\n",
        ")\n",
        "trec_predictions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jX30rjV-eMk"
      },
      "outputs": [],
      "source": [
        "# Evaluate the predictions for the TREC data\n",
        "trec_eval_df = evaluate_relevance_predictions(\n",
        "    trec_predictions_df, trec_qrels_df\n",
        ").set_index([\"query_id\", \"doc_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1WepXtMBWVs"
      },
      "outputs": [],
      "source": [
        "def print_random_disagreement(\n",
        "    _eval_df, _queries_df, _docs_df, pred_score, rel_score\n",
        ") -> None:\n",
        "    _df = _eval_df.loc[(_eval_df[\"predicted\"] == pred_score) & (_eval_df[\"relevance\"] == rel_score)]\n",
        "    if _df.empty:\n",
        "        print(\n",
        "            f\"No disagreements found for predicted score {pred_score} and relevance score {rel_score}\"\n",
        "        )\n",
        "        return\n",
        "    print(\n",
        "        f\"Found {len(_df)} disagreements for predicted score {pred_score} and relevance score {rel_score}\"\n",
        "    )\n",
        "\n",
        "    # print a random disagreement\n",
        "    qid, docid = _df.sample(n=1).index[0]\n",
        "\n",
        "    print(\n",
        "        f\"The following pair has relevance scores of:\\n{_eval_df.loc[(qid, docid), ['relevance', 'predicted']]}\\n\"\n",
        "    )\n",
        "    print(f'Query: {_queries_df.loc[qid, \"text\"]}')\n",
        "    printw(f'Document: {_docs_df.loc[docid, \"text\"]}')\n",
        "\n",
        "\n",
        "print_random_disagreement(\n",
        "    trec_eval_df,\n",
        "    trec_queries_df,\n",
        "    trec_docs_df,\n",
        "    pred_score=trec_min_rel,\n",
        "    rel_score=trec_max_rel,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv2xYs-YozMr"
      },
      "outputs": [],
      "source": [
        "# test with 'gemini-flash-2' takes 3-4 minutes\n",
        "trec_results_2, trec_reasoning_results_2, trec_raw_responses_2 = (\n",
        "    generate_relevance_predictions(\n",
        "        trec_qrels_df,\n",
        "        trec_docs_df,\n",
        "        trec_queries_df,\n",
        "        prompt=umbrella_prompt,\n",
        "        model=MODEL[\"gemini-flash-2\"],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG80var0pRGB"
      },
      "outputs": [],
      "source": [
        "compute_total_tokens_used(trec_raw_responses_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEOxZIBcpUrp"
      },
      "outputs": [],
      "source": [
        "trec_predictions_df_2 = parse_results_to_df(\n",
        "    trec_results_2,\n",
        "    trec_reasoning_results_2,\n",
        "    scores_range=(trec_min_rel, trec_max_rel),\n",
        ")\n",
        "trec_predictions_df_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoU3xdibpbG1"
      },
      "outputs": [],
      "source": [
        "# Evaluate the predictions for the TREC data\n",
        "trec_eval_df_2 = evaluate_relevance_predictions(\n",
        "    trec_predictions_df_2, trec_qrels_df\n",
        ").set_index([\"query_id\", \"doc_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ck9ZC-LGa--"
      },
      "outputs": [],
      "source": [
        "print_random_disagreement(\n",
        "    trec_eval_df_2, trec_queries_df, trec_docs_df, pred_score=trec_max_rel, rel_score=trec_min_rel\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMItCBKcujgn"
      },
      "outputs": [],
      "source": [
        "# test with 'OpenAI: GPT-4o (2024-11-20)' takes 3-4 minutes\n",
        "trec_results_3, trec_reasoning_results_3, trec_raw_responses_3 = (\n",
        "    generate_relevance_predictions(\n",
        "        trec_qrels_df,\n",
        "        trec_docs_df,\n",
        "        trec_queries_df,\n",
        "        prompt=umbrella_prompt,\n",
        "        model=\"openai/gpt-4o-2024-11-20\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BsnStLtwoW3"
      },
      "outputs": [],
      "source": [
        "compute_total_tokens_used(trec_raw_responses_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Orr6A2BwwZGO"
      },
      "outputs": [],
      "source": [
        "trec_predictions_df_3 = parse_results_to_df(\n",
        "    trec_results_3,\n",
        "    trec_reasoning_results_3,\n",
        "    scores_range=(trec_min_rel, trec_max_rel),\n",
        ")\n",
        "trec_predictions_df_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvczrsBJwr3m"
      },
      "outputs": [],
      "source": [
        "trec_eval_df_3 = evaluate_relevance_predictions(\n",
        "    trec_predictions_df_3, trec_qrels_df\n",
        ").set_index([\"query_id\", \"doc_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x06j_Hyw13d"
      },
      "outputs": [],
      "source": [
        "print_random_disagreement(\n",
        "    trec_eval_df_3,\n",
        "    trec_queries_df,\n",
        "    trec_docs_df,\n",
        "    pred_score=trec_min_rel,\n",
        "    rel_score=trec_max_rel,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}